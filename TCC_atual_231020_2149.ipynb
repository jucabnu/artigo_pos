{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TCC atual 231020-2149.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP6340X3cNlS8zQD53rUd+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jucabnu/artigo_pos/blob/main/TCC_atual_231020_2149.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb8pGGwo86Uh"
      },
      "source": [
        "# Preparando a estação de trabalho no Google Colab \n",
        "\n",
        "# instalando as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# configurando  as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        " \n",
        "# tornando o pyspark inicializável\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "# criando a sessão Spark e configurando os nós de cluster\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "   .master(\"local\") \\\n",
        "   .appName(\"Linear Regression Model\") \\\n",
        "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
        "   .getOrCreate()\n",
        "   \n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGCfZubOF1ew",
        "outputId": "35b956af-118c-4405-bb4f-ab3139f2976d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# https://raw.githubusercontent.com/jucabnu/artigo_pos/main/arquivo.csv\n",
        "!rm *.csv\n",
        "op = input('Como deseja enviar o arquivo? (1 - URI / 2 - Upload): ')\n",
        "if op == '1':\n",
        "  site = str(input('Informe a URI: '))\n",
        "  !wget --quiet --show progress $site\n",
        "else:\n",
        "  from google.colab import files\n",
        "  files.upload()\n",
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Como deseja enviar o arquivo? (1 - URI / 2 - Upload): 1\n",
            "Informe a URI: https://raw.githubusercontent.com/jucabnu/artigo_pos/main/arquivo.csv\n",
            "arquivo.csv         100%[===================>]  53.90K  --.-KB/s    in 0.008s  \n",
            "arquivo.csv  spark-2.4.4-bin-hadoop2.7\n",
            "sample_data  spark-2.4.4-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdNNQ5UO-ObN",
        "outputId": "c3156de7-c254-4231-ada4-3f4b3007f8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "# Montagem do RDD (Resilient Distributed Dataset)\n",
        "\n",
        "# Atribuindo o arquivo CSV ao RDD\n",
        "rdd = sc.textFile('arquivo.csv')\n",
        "\n",
        "# Separação dos dados pelo separador do SCV\n",
        "rdd = rdd.map(lambda line: line.split(\",\"))\n",
        "\n",
        "# Importação do pyspark.sql para a montagem do DF (Data Frame)\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Mapeando o RDD para o DF\n",
        "df = rdd.map(lambda line: Row(Versao=line[0], Qualidade=line[1])).toDF()\n",
        "\n",
        "# Apresentando os 20 primeiros registros e o schema para verificação\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------+\n",
            "|Qualidade|Versao|\n",
            "+---------+------+\n",
            "|        2|     3|\n",
            "|       51|     2|\n",
            "|       53|    10|\n",
            "|       71|     7|\n",
            "|       69|     1|\n",
            "|       58|     8|\n",
            "|       73|     3|\n",
            "|       63|     5|\n",
            "|       51|     2|\n",
            "|       58|     2|\n",
            "|       25|     9|\n",
            "|        1|     2|\n",
            "|       80|     6|\n",
            "|       56|     5|\n",
            "|       78|     7|\n",
            "|       64|     3|\n",
            "|       45|     1|\n",
            "|       75|     7|\n",
            "|       12|     1|\n",
            "|       59|     5|\n",
            "+---------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- Qualidade: string (nullable = true)\n",
            " |-- Versao: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuaQ4yokCDK1",
        "outputId": "9bb65822-7d59-4d47-daf7-c3c9a58c4e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Importando todos os métodos de pyspark.sql.types\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Função para converter o tipo de dados das colunas do DF.\n",
        "def convertColumn(df, names, newType):\n",
        "    for name in names: \n",
        "        df = df.withColumn(name, df[name].cast(newType))\n",
        "    return df \n",
        "\n",
        "columns = ['Qualidade', 'Versao']\n",
        "\n",
        "# Chamando a função e passando o parâmetro para conversão em float\n",
        "df = convertColumn(df, columns, FloatType())\n",
        "\n",
        "# Alguns métodos úteis\n",
        "\n",
        "# df.select('Qualidade').show(10)\n",
        "# df.groupBy(\"Qualidade\").count().sort(\"Qualidade\",ascending=False).show()\n",
        "\n",
        "# Metodo para verificar o resumo do data frame, com a contagem, média, desvio padrão, mínimo e máximo de cada elemento\n",
        "df.describe().show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+-----------------+\n",
            "|summary|         Qualidade|           Versao|\n",
            "+-------+------------------+-----------------+\n",
            "|  count|             11000|            11000|\n",
            "|   mean| 50.59581818181818|5.459727272727273|\n",
            "| stddev|29.018550624661852| 2.88120019967071|\n",
            "|    min|               1.0|              1.0|\n",
            "|    max|             100.0|             10.0|\n",
            "+-------+------------------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS7kV4gWCq7c",
        "outputId": "7abd5e60-7490-4090-9616-c68b5fad1bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Preparando o DF para aplicação do Machine Learning.\n",
        "\n",
        "# Importação da biblioteca 'DenseVector'\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "\n",
        "# Definição do 'input_data' \n",
        "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
        "\n",
        "# Substituindo o DF pelo novo Data Frame preparado\n",
        "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
        "\n",
        "# Importando e inicializando o 'StandardScaler'\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "# Ajustando o DataFrame para Scaler e transformando os dados.\n",
        "scaler = standardScaler.fit(df)\n",
        "scaled_df = scaler.transform(df)\n",
        "\n",
        "# Inspecionar o resultado\n",
        "scaled_df.take(2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(label=2.0, features=DenseVector([3.0]), features_scaled=DenseVector([1.0412])),\n",
              " Row(label=51.0, features=DenseVector([2.0]), features_scaled=DenseVector([0.6942]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZPvhzY1Dhst",
        "outputId": "1607b739-9c51-4735-ad9d-000941cec6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Aplicação prática de Machine Learning\n",
        "\n",
        "# Dividindo os dados em conjuntos de 'treino' e 'testes'\n",
        "train_data, test_data = scaled_df.randomSplit([.75,.25],seed=1234)\n",
        "\n",
        "# Importando e inicializando o método para Regressão Linear\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
        "\n",
        "# Ajustando os dados no modelo\n",
        "linearModel = lr.fit(train_data)\n",
        "\n",
        "# Gerando as predições\n",
        "predicted = linearModel.transform(test_data)\n",
        "\n",
        "# Extraind as previões e rotulando os valores corretos \"conhecidos\"\n",
        "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
        "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
        "\n",
        "# Agrupando as predições e as etiquetas numa lista\n",
        "predictionAndLabel = predictions.zip(labels).collect()\n",
        "\n",
        "# Imprimindo as 5 primeiras instâncias das predições e etiquetas \n",
        "predictionAndLabel[:5]\n",
        "\n",
        "# Mostrando o coeficiente do modelo\n",
        "print('Coeficiente do modelo: ', linearModel.coefficients)\n",
        "\n",
        "# Mostrando a intercessão do modelo\n",
        "print('Intercessão do modelo: ', linearModel.intercept)\n",
        "\n",
        "# Mostrando o RMSE. Erro de raiz quadrático médio (root-mean-square error)\n",
        "print('RMSE: ', linearModel.summary.rootMeanSquaredError)\n",
        "\n",
        "# Mostrando o R2 (coeficiente de determinação)\n",
        "print('R2: ', linearModel.summary.r2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coeficiente do modelo:  [0.06328203733852185]\n",
            "Intercessão do modelo:  49.975621959605604\n",
            "RMSE:  29.131255605850733\n",
            "R2:  3.904094749518361e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm1Km_ihD-M1"
      },
      "source": [
        "# Parando a instância do Spark e finalizando a aplicação\n",
        "spark.stop()\n"
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}